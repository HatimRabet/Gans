{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Import all libraires that will be used </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ImageDataset, generate_CutMix_samples, loss_decoder, loss_encoder, loss_regularization\n",
    "from models import Unet_Discriminator, Unet_Generator\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "from utils import (\n",
    "    ImageDataset,\n",
    "    generate_CutMix_samples,\n",
    "    loss_decoder,\n",
    "    loss_encoder,\n",
    "    loss_regularization,\n",
    ")\n",
    "\n",
    "from models import (\n",
    "    Unet_Discriminator,\n",
    "    Unet_Generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Initializing the paintings dataset </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44838_2014.jpg\n",
      "True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PaintingsDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m      9\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((image_size,image_size)), \u001b[38;5;66;03m# Standardizing the size of the images\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(), \u001b[38;5;66;03m# Transforming to tensor\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.5\u001b[39m),(\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.5\u001b[39m)) \u001b[38;5;66;03m# Normalizing\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Initializing the dataset\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m training_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPaintingsDataset\u001b[49m(image_dir, transform, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(training_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PaintingsDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Building the dataset\n",
    "image_dir = '../data/anime/images_2'\n",
    "image_size = 64\n",
    "\n",
    "f = os.listdir(image_dir)[0]\n",
    "print(f)\n",
    "print(os.path.isfile(os.path.join(image_dir, f)))\n",
    "\n",
    "# Transformations to normalize the data before dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size,image_size)), # Standardizing the size of the images\n",
    "    transforms.ToTensor(), # Transforming to tensor\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)) # Normalizing\n",
    "])\n",
    "\n",
    "\n",
    "# Initializing the dataset\n",
    "training_dataset = ImageDataset(image_dir, transform, limit=10000)\n",
    "print(f\"Dataset contains {len(training_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Sampling an element from the dataset and plotting it </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling randomly an element from the dataset\n",
    "n = len(training_dataset)\n",
    "integer = random.randint(0,n)\n",
    "\n",
    "# Sampled image\n",
    "image = training_dataset[integer].numpy()*0.5 + 0.5 # De-normalizing the image\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.imshow(np.transpose(image, (1, 2, 0)))  \n",
    "plt.axis('off')  # Hide axes\n",
    "plt.title('Sample Painting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Initializing the parameters of the model </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's parameters\n",
    "latent_dim = 100\n",
    "channels_out = 3\n",
    "channels_in = 3\n",
    "\n",
    "# Intializing the models\n",
    "G_unet = Unet_Generator(latent_dim, channels_out, num_upsamples=4)\n",
    "D_unet = Unet_Discriminator(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting randomly a sample from the dataset to plot the examples\n",
    "G_unet\n",
    "integer = random.randint(0, n)\n",
    "image = training_dataset[integer]\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn(63, latent_dim, 64, 64)  # Batch of 16 noise vectors\n",
    "\n",
    "# Generate images\n",
    "fake_images = G_unet(noise)[0].unsqueeze(0) # Batch of 16 fake images\n",
    "print(fake_images.shape)\n",
    "\n",
    "# Generating another example from the dataset\n",
    "integer = random.randint(0,n)\n",
    "real_image = training_dataset[integer]\n",
    "\n",
    "# Unsqueezing to match the shape of input\n",
    "real_image = real_image.unsqueeze(0)\n",
    "print(real_image.shape)\n",
    "\n",
    "# Generating the cutmix image\n",
    "ratio, cutmixed, cutmixed_decoded, target_a, target_b, bbx1, bbx2, bby1, bby2 = generate_CutMix_samples(real_image, fake_images, D_unet, device=torch.device('cpu'))\n",
    "\n",
    "# Preprocessing to de-normalize\n",
    "cut_mixed = cutmixed.squeeze(0).detach().numpy()*0.5 +0.5\n",
    "image = fake_images.squeeze(0).detach().numpy()*0.5 +0.5\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(np.transpose(cut_mixed, (1, 2, 0)))  \n",
    "plt.axis('off')  # Hide axes\n",
    "plt.title('Sample Painting Image After Cut-Mix Operation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Sampling a vector to plot the fake image generated by the Generator </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random noise\n",
    "noise = torch.randn(64, latent_dim, 64, 64)  # Batch of 16 noise vectors\n",
    "\n",
    "# Generate images\n",
    "fake_images = G_unet(noise)\n",
    "\n",
    "# De-normalizing the image\n",
    "image_generated = fake_images[0].detach().numpy().reshape((3,64,64))*0.5 + 0.5\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(np.transpose(image_generated, (1, 2, 0)))  \n",
    "plt.axis('off')  # Hide axes\n",
    "plt.title('Sample a generated image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Let build the training loop</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "default_batch_size = 64\n",
    "dataloader = DataLoader(training_dataset, batch_size=default_batch_size)\n",
    "\n",
    "loss_options = [\"w\",\"bce\"]\n",
    "loss_option = loss_options[1]\n",
    "\n",
    "experiment = 4\n",
    "\n",
    "output_dir = f\"generated_samples/cutmix/experiment_{experiment}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_channels = 3\n",
    "channels_out = input_channels\n",
    "n_classes = 2\n",
    "k = 5  # number of discriminator updates per generator update\n",
    "epochs =  1000\n",
    "latent_dim = 128  # Dimension of the latent space\n",
    "lambda_gp = 1  # Gradient penalty weight\n",
    "lambda_div = 5  # Diversity loss weight\n",
    "p_mix = 0 # Probability of generating the cutmix images\n",
    "steps_mix = 60\n",
    "p_max = 0.5\n",
    "\n",
    "\n",
    "d_lr = 1e-4 if loss_option == \"w\" else 2e-4  # Base learning rate\n",
    "g_lr = 2e-4 if loss_option == \"w\" else 2e-4  # Base learning rate\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Initialize models\n",
    "G_unet = Unet_Generator(latent_dim, channels_out, num_upsamples=4).to(device)  \n",
    "D_unet = Unet_Discriminator(input_channels, n_classes=n_classes).to(device)\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion_encoder = loss_encoder\n",
    "criterion_decoder = loss_decoder\n",
    "criterion_regular = loss_regularization\n",
    "\n",
    "optimizer_g = torch.optim.Adam(G_unet.parameters(), lr=g_lr, betas=(0.0, 0.9) if loss_option == \"w\" else (0.5, 0.999))\n",
    "optimizer_d = torch.optim.Adam(D_unet.parameters(), lr=d_lr, betas=(0.0, 0.9) if loss_option == \"w\" else (0.5, 0.999))\n",
    "scheduler_g = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_g, T_max=epochs)\n",
    "scheduler_d = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_d, T_max=epochs)\n",
    "\n",
    "# FID Evaluation setup\n",
    "epoch_fid = epochs // 20  # Evaluate FID every `epoch_eval` epochs\n",
    "epoch_sampling = 10 # Save generated samples every `epoch_sampling` epochs\n",
    "\n",
    "fid = FrechetInceptionDistance(feature=192, reset_real_features=False, normalize=True)\n",
    "n_samples = 500\n",
    "\n",
    "N = len(training_dataset) \n",
    "indices = random.sample(range(N), n_samples)\n",
    "real_images_eval = torch.stack([training_dataset[idx] for idx in indices])\n",
    "fid.update(real_images_eval, real=True)\n",
    "\n",
    "\n",
    "# Track losses and FID values\n",
    "FID_values = []\n",
    "D_loss = []\n",
    "G_loss = []\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    start_time = time.time()\n",
    "    iter_count = 0\n",
    "    cutmix_count = 0\n",
    "\n",
    "    # Turn dataloader into an iterator for this epoch\n",
    "    data_iter = iter(dataloader)\n",
    "    ################################################################\n",
    "    # 1) DISCRIMINATOR UPDATES — do k iterations\n",
    "    ################################################################  \n",
    "    while True:\n",
    "        for _ in range(k):\n",
    "            try:\n",
    "                # Get next batch of real images\n",
    "                real_images = next(data_iter).to(device)\n",
    "            except StopIteration:\n",
    "                # If we've exhausted the dataloader, end this epoch\n",
    "                break\n",
    "            \n",
    "            # Labels for real and fake images\n",
    "            batch_size = real_images.size(0)\n",
    "            \n",
    "            labels_real = torch.full((batch_size, 1), 0.9, device=device)\n",
    "            labels_fake = torch.full((batch_size, 1), 0.1, device=device)\n",
    "            \n",
    "            labels_real_pixel = torch.full((batch_size, 1, 64, 64), 0.9, device=device)\n",
    "            labels_fake_pixel = torch.full((batch_size, 1, 64, 64), 0.1, device=device)\n",
    "            \n",
    "\n",
    "            # Train discriminator with real images\n",
    "            optimizer_d.zero_grad()\n",
    "\n",
    "            # Add small noise to real images\n",
    "            real_noisy = real_images + torch.randn_like(real_images) * 0.01\n",
    "\n",
    "            output_real = D_unet(real_noisy)\n",
    "            output_1_real = output_real[0]\n",
    "            output_2_real = output_real[1]\n",
    "            \n",
    "            loss_d_real_encoder = criterion_encoder(output_1_real, labels_real)\n",
    "            loss_d_real_decoder = criterion_decoder(output_2_real, labels_real_pixel)\n",
    "\n",
    "            # Train discriminator with fake images\n",
    "            noise = torch.randn(batch_size, latent_dim, 64, 64, device=device)\n",
    "            fake_images = G_unet(noise).detach() \n",
    "            \n",
    "            fake_noisy = fake_images + torch.randn_like(fake_images) * 0.01\n",
    "\n",
    "            # Forward pass of the fake images through the U-Net-based discriminator\n",
    "            output_fake = D_unet(fake_noisy)  \n",
    "            output_1_fake = output_fake[0]  # First part of the output (for encoder loss)\n",
    "            output_2_fake = output_fake[1]  # Second part of the output (for decoder loss)\n",
    "\n",
    "            # Calculate the loss for the fake images for both encoder and decoder outputs\n",
    "            loss_d_fake_encoder = criterion_encoder(output_1_fake, labels_fake)  # Loss for encoder using fake images\n",
    "            loss_d_fake_decoder = criterion_decoder(output_2_fake, labels_fake_pixel)  # Loss for decoder using fake pixel labels\n",
    "            \n",
    "            alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "            d_interpolates = alpha * real_images + (1 - alpha) * fake_images\n",
    "            d_interpolates.requires_grad = True\n",
    "            d_out = D_unet(d_interpolates)[0]\n",
    "            gradients = torch.autograd.grad(outputs=d_out, inputs=d_interpolates,\n",
    "                                            grad_outputs=torch.ones_like(d_out, device=device),\n",
    "                                            create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "            gradient_penalty = lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "            # Total loss for discriminator combining real and fake image losses\n",
    "            loss_d = loss_d_real_encoder + loss_d_fake_encoder + loss_d_real_decoder + loss_d_fake_decoder + gradient_penalty\n",
    "            \n",
    "            # Apply CutMix augmentation with a probability of p_mix\n",
    "            if random.random() < p_mix:\n",
    "                # Generate CutMix samples by mixing real and fake images\n",
    "                ratio, cutmix_images, cutmix_decoded, target_a, target_b, bbx1, bbx2, bby1, bby2 = generate_CutMix_samples(real_images, fake_images, D_unet, device)\n",
    "                \n",
    "                labels_cutmix = torch.full((batch_size, 1), 0.9, device=device)\n",
    "                labels_cutmix_pixel = torch.full((batch_size, 1, 64, 64), 0.9, device=device)\n",
    "                \n",
    "                for x1, x2, y1, y2 in zip(bbx1, bbx2, bby1, bby2):\n",
    "                    labels_cutmix_pixel[:,:,x1:x2, y1:y2] = 0.1  # Set labels to 0.1 inside the bounding box for the mixed area\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                # Forward pass of the CutMix images through the U-Net-based discriminator\n",
    "                output_cutmix = D_unet(cutmix_images)  \n",
    "                output_1_cutmix = output_cutmix[0]  # First part of the output (for encoder loss)\n",
    "                output_2_cutmix = output_cutmix[1]  # Second part of the output (for decoder loss)\n",
    "\n",
    "                # Calculate the loss for the CutMix images for encoder, decoder, and regularization\n",
    "                loss_d_encoder_cutmix = criterion_encoder(output_1_cutmix, labels_cutmix)  # Encoder loss with CutMix images\n",
    "                loss_d_decoder_cutmix = criterion_decoder(output_2_cutmix, labels_cutmix_pixel)  # Decoder loss with pixel labels\n",
    "                \n",
    "                loss_d_cutmix_regular = criterion_regular(output_2_cutmix, cutmix_decoded)  # Regularization loss for the decoded mixed region\n",
    "\n",
    "                # Add CutMix losses to the total discriminator loss\n",
    "                loss_d += loss_d_encoder_cutmix + loss_d_decoder_cutmix + loss_d_cutmix_regular\n",
    "                \n",
    "            \n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            D_loss.append(loss_d.item())\n",
    "\n",
    "        else:\n",
    "            ################################################################\n",
    "            # 2) GENERATOR UPDATE — 1 iteration\n",
    "            ################################################################\n",
    "            for p in D_unet.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "            labels_real = torch.full((default_batch_size, 1), 0.9, device=device)\n",
    "            labels_real_pixel = torch.full((default_batch_size, 1, 64, 64), 0.9, device=device)\n",
    "            \n",
    "            noise = torch.randn(default_batch_size, latent_dim, 64, 64, device=device)\n",
    "            fake_images = G_unet(noise)\n",
    "\n",
    "            optimizer_g.zero_grad()\n",
    "\n",
    "            output_1, output_2 = D_unet(fake_images)\n",
    "            loss_g_encoder = criterion_encoder(output_1, labels_real)  # Generator encoder loss (fake -> real)\n",
    "            loss_g_decoder = criterion_decoder(output_2, labels_real_pixel)  # Generator decoder loss (fake -> real pixels)\n",
    "\n",
    "\n",
    "            loss_g = loss_g_decoder + loss_g_encoder\n",
    "            G_loss.append(loss_g.item())\n",
    "\n",
    "            # Backpropagate the generator loss and update generator's parameters\n",
    "            loss_g.backward()  # Calculate gradients for the generator\n",
    "            optimizer_g.step()  # Update the generator's weights\n",
    "\n",
    "            for p in D_unet.parameters():\n",
    "                p.requires_grad = True\n",
    "         # Go back to the while loop for next set of k updates + 1 gen update\n",
    "            continue\n",
    "\n",
    "        # If we did break out of the for-loop (StopIteration),\n",
    "        # it means we've run out of data for this epoch.\n",
    "        break\n",
    "    \n",
    "    \n",
    "     # Evaluate FID every `epoch_eval` epochs\n",
    "    if epoch % epoch_fid == 0:\n",
    "        G_unet.eval()  # Set generator to eval mode for FID computation\n",
    "        fake_images_eval = []\n",
    "        with torch.no_grad():\n",
    "        # Generate evaluation images\n",
    "            noise_eval = torch.randn(n_samples, latent_dim, 64, 64, device=device)\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                fake_images_chunk = G_unet(noise_eval[i:i+batch_size]).to('cpu')\n",
    "                fid.update(fake_images_chunk, real=False)\n",
    "                fake_images_eval.append(fake_images_chunk)\n",
    "\n",
    "            fid_value = fid.compute().item()\n",
    "            FID_values.append(fid_value)\n",
    "\n",
    "            print(f'FID after epoch {epoch}: {fid_value}')\n",
    "\n",
    "        # Save a grid of generated samples\n",
    "        fake_images_eval = torch.cat(fake_images_eval, dim=0)\n",
    "        grid = vutils.make_grid(fake_images_eval[:64], normalize=True, scale_each=True)\n",
    "        sample_path = os.path.join(output_dir, f\"epoch_{epoch:03d}.png\")\n",
    "        vutils.save_image(grid, sample_path)\n",
    "        print(f\"Sample images saved to {sample_path}\")\n",
    "        \n",
    "        del fake_images_eval\n",
    "        torch.cuda.empty_cache()\n",
    "        fid.reset()\n",
    "        G_unet.train()\n",
    "        \n",
    "    if epoch % epoch_sampling == 0:\n",
    "        G_unet.eval()  # Set generator to eval mode for FID computation\n",
    "        \n",
    "        # Generate evaluation images\n",
    "        with torch.no_grad():\n",
    "            noise_eval = torch.randn(64, latent_dim, 64, 64, device=device)\n",
    "            fake_images_eval = G_unet(noise_eval).to(torch.device('cpu'))\n",
    "        \n",
    "        # Save a grid of generated samples\n",
    "        grid = vutils.make_grid(fake_images_eval, normalize=True, scale_each=True)\n",
    "        sample_path = os.path.join(output_dir, f\"epoch_{epoch:03d}.png\")\n",
    "        vutils.save_image(grid, sample_path)\n",
    "        print(f\"Sample images saved to {sample_path}\")\n",
    "        \n",
    "        del fake_images_eval\n",
    "        torch.cuda.empty_cache()\n",
    "        G_unet.train()\n",
    "        \n",
    "        # Track losses\n",
    "        D_loss.append(loss_d.item())\n",
    "        G_loss.append(loss_g.item())\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Update the probability of applying CutMix augmentation, gradually increasing to a maximum of 0.5\n",
    "    scheduler_g.step()\n",
    "    scheduler_d.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch}/{epochs}] | Loss D: {loss_d.item()} | Loss G: {loss_g.item()} | Gradient Penalty: {gradient_penalty.item()}')\n",
    "    print(f'Epoch duration: {end_time - start_time:.2f}s')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f\"weights/cutmix/experiment_{experiment}\"\n",
    "\n",
    "\n",
    "torch.save(D_unet.state_dict(), f\"{model_dir}/d_unet.pth\")\n",
    "torch.save(G_unet.state_dict(), f\"{model_dir}/g_unet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Generating some examples using the trained generator </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random noise\n",
    "noise = torch.randn(1, latent_dim, 64, 64)  # Batch of 16 noise vectors\n",
    "\n",
    "G_unet = G_unet.to('cpu')\n",
    "# Generate images\n",
    "fake_images = G_unet(noise)\n",
    "\n",
    "print(fake_images.size())  # Should be (16, img_channels, height, width)\n",
    "\n",
    "image_noise = fake_images.detach().numpy().reshape((3,64,64))*0.5 + 0.5\n",
    "\n",
    "plt.imshow(np.transpose(image_noise, (1, 2, 0)))\n",
    "plt.title(\"Sampling a generated image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Plotting Losses <strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming G_loss and D_loss are defined\n",
    "ng = len(G_loss)\n",
    "nd = len(D_loss)\n",
    "time_steps_g = [i for i in range(ng)]\n",
    "time_steps_d = [i for i in range(nd)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Generator Loss\n",
    "plt.plot(time_steps_g, G_loss, label='Generator Loss', color='darkorange', linestyle='-', linewidth=2)\n",
    "\n",
    "# Plot Discriminator Loss\n",
    "plt.plot(time_steps_d, D_loss, label='Discriminator Loss', color='royalblue', linestyle='--', linewidth=2)\n",
    "\n",
    "# Add grid, labels, and title\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xlabel('Time Steps', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Loss Evolution of Generator and Discriminator', fontsize=14)\n",
    "\n",
    "# Adding legend\n",
    "plt.legend(loc='upper right', fontsize=11)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Plotting FID values <strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(FID_values)\n",
    "time_steps = [i for i in range(n)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot FID values\n",
    "plt.plot(time_steps, FID_values, label='Generator FID', color='darkorange', linestyle='-', linewidth=2)\n",
    "\n",
    "\n",
    "# Add grid, labels, and title\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xlabel('Time Steps', fontsize=12)\n",
    "plt.ylabel('FID', fontsize=12)\n",
    "plt.title('FID Evolution through training', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plots\n",
    "plots_dir = f\"plots/cutmix/experiment_{experiment}\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "np.savetxt(f\"{plots_dir}/g_loss.txt\", np.array(G_loss))\n",
    "np.savetxt(f\"{plots_dir}/d_loss.txt\", np.array(D_loss))\n",
    "\n",
    "np.savetxt(f\"{plots_dir}/fid_values.txt\", np.array(FID_values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
