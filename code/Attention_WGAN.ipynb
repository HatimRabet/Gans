{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Import all libraires that will be used </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from models import AttentionUNetDiscriminator, AttentionUNetGenerator\n",
    "from utils import ImageDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Initializing the paintings dataset </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataset\n",
    "image_dir = '../data/anime/images_2'\n",
    "image_size = 64\n",
    "f = os.listdir(image_dir)[0]\n",
    "print(f)\n",
    "print(os.path.isfile(os.path.join(image_dir, f)))\n",
    "\n",
    "# Transformations to normalize the data before dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size,image_size)), # Standardizing the size of the images\n",
    "    transforms.ToTensor(), # Transforming to tensor\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)) # Normalizing\n",
    "])\n",
    "\n",
    "\n",
    "# Initializing the dataset\n",
    "training_dataset = ImageDataset(image_dir, transform, limit=10000)\n",
    "print(f\"Dataset contains {len(training_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Sampling an element from the dataset and plotting it </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling randomly an element from the dataset\n",
    "n = len(training_dataset)\n",
    "integer = random.randint(0,n)\n",
    "\n",
    "# Sampled image\n",
    "image = training_dataset[integer].numpy()*0.5 + 0.5 # De-normalizing the image\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.imshow(np.transpose(image, (1, 2, 0)))  \n",
    "plt.axis('off')  # Hide axes\n",
    "plt.title('Sample Painting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Intializing the Dataloader </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Initializing the parameters of the model </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's parameters\n",
    "latent_dim = 100\n",
    "channels_out = 3\n",
    "channels_in = 3\n",
    "\n",
    "# Intializing the models\n",
    "G = AttentionUNetGenerator(latent_dim, channels_out)\n",
    "D = AttentionUNetDiscriminator(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Sampling a vector to plot the fake image generated by the Generator </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random noise\n",
    "latent_dim = 100  # Latent space dimension\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn(16, latent_dim, image_size, image_size)  # Noise input for generator\n",
    "# Generate images\n",
    "fake_images = G(noise)  # Output shape: [16, 3, 128, 128]\n",
    "\n",
    "# De-normalize and reshape the first generated image\n",
    "image_generated = fake_images[0].detach().cpu().numpy()  # Select first image in batch\n",
    "image_generated = image_generated * 0.5 + 0.5  # De-normalize to [0, 1]\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(np.transpose(image_generated, (1, 2, 0)))  # Convert to (H, W, C)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.title('Sample Generated Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Let build the training loop</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "def diversity_loss(fake_images, feature_extractor):\n",
    "    # Extract features using a pretrained model\n",
    "    features = feature_extractor(fake_images)  # Shape: (batch_size, feature_dim)\n",
    "\n",
    "    # Normalize features to unit vectors\n",
    "    features = F.normalize(features, p=2, dim=1)\n",
    "\n",
    "    # Compute pairwise cosine similarity\n",
    "    similarity_matrix = torch.matmul(features, features.T)  # Shape: (batch_size, batch_size)\n",
    "\n",
    "    # Remove diagonal elements (self-similarity)\n",
    "    batch_size = similarity_matrix.size(0)\n",
    "    mask = torch.eye(batch_size, device=similarity_matrix.device).bool()\n",
    "    diversity_penalty = similarity_matrix.masked_fill(mask, 0).mean()\n",
    "\n",
    "    return diversity_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(D, real, fake,device='cuda'):\n",
    "    # Compute random weight for interpolation\n",
    "    alpha = torch.rand(real.size(0), 1, 1, 1).to(device)\n",
    "\n",
    "    # Interpolate real and fake images\n",
    "    interpolated = (alpha * real + (1 - alpha) * fake).requires_grad_()\n",
    "\n",
    "    # Compute output of the critic\n",
    "    prob_interpolated = D(interpolated)\n",
    "\n",
    "    # Compute gradients of the critic with respect to the interpolated images\n",
    "    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                                    grad_outputs=torch.ones_like(prob_interpolated),\n",
    "                                    create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # Compute gradient penalty\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(p=2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "loss_options = [\"w\",\"bce\"]\n",
    "loss_option = loss_options[0]\n",
    "\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(training_dataset, batch_size=batch_size)\n",
    "\n",
    "image_size = 64\n",
    "experiment = 3\n",
    "\n",
    "output_dir = f\"generated_samples/wgan/experiment_{experiment}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "input_channels = 3\n",
    "channels_out = input_channels\n",
    "n_classes = 2\n",
    "k = 5  # number of critic updates per generator update\n",
    "latent_dim = 128 # Dimension of latent space\n",
    "epochs = 1000 # Number of epochs\n",
    "lambda_gp = 10  # Gradient penalty weight\n",
    "lambda_div = 5  # Diversity loss weight\n",
    "\n",
    "d_lr = 1e-4 if loss_option == \"w\" else 2e-4  # Base learning rate\n",
    "g_lr = 2e-4 if loss_option == \"w\" else 2e-4  # Base learning rate\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# feature extractor to compute diversity loss\n",
    "feature_extractor = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "feature_extractor.eval().to(device)\n",
    "\n",
    "# Initialize models (Use Attention U-Net GAN models)\n",
    "G_attention = AttentionUNetGenerator(latent_dim, channels_out).to(device)  # Latent space maps to input\n",
    "\n",
    "if loss_option == \"w\":\n",
    "    D_attention = AttentionUNetDiscriminator(input_channels, features=64, use_sigmoid=False).to(device)  # Wasserstein discriminator\n",
    "elif loss_option == \"bce\":\n",
    "    D_attention = AttentionUNetDiscriminator(input_channels, features=64).to(device)  # PatchGAN discriminator\n",
    "\n",
    "# Loss function and optimizers\n",
    "if loss_option == \"bce\":\n",
    "    criterion = torch.nn.BCELoss()  # Binary Cross-Entropy for GANs\n",
    "\n",
    "optimizer_g = torch.optim.Adam(G_attention.parameters(), lr=g_lr, betas=(0.0, 0.9) if loss_option == \"w\" else (0.5, 0.999))\n",
    "optimizer_d = torch.optim.Adam(D_attention.parameters(), lr=d_lr, betas=(0.0, 0.9) if loss_option == \"w\" else (0.5, 0.999))\n",
    "\n",
    "# FID Evaluation setup\n",
    "epoch_fid = 100  # Evaluate FID every `epoch_eval` epochs\n",
    "epoch_sampling = 10 # Save generated samples every `epoch_sampling` epochs\n",
    "\n",
    "\n",
    "fid = FrechetInceptionDistance(feature=192, reset_real_features=False, normalize=True)\n",
    "n_samples = 1000\n",
    "\n",
    "# Ensure your dataset of real paintings is available\n",
    "N = len(training_dataset)  # Assuming `paintings` is a tensor of real images\n",
    "\n",
    "# Batch update the FID score\n",
    "indices = random.sample(range(N), n_samples)\n",
    "real_images_eval = torch.stack([training_dataset[idx] for idx in indices])\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    real_images_chunk = real_images_eval[i:i+batch_size].to('cpu')\n",
    "    fid.update(real_images_chunk, real=True)\n",
    "del real_images_eval\n",
    "\n",
    "# Track losses and FID values\n",
    "FID_values = []\n",
    "D_loss = []\n",
    "G_loss = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    generator_count = 0\n",
    "    generator_loss = 0\n",
    "    \n",
    "    critic_count = 0\n",
    "    critic_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        ################################################################\n",
    "        # 1) CRITIC (DISCRIMINATOR) UPDATES \n",
    "        ################################################################\n",
    "        \n",
    "        # Get next batch of real images\n",
    "        real_images = batch.to(device)\n",
    "        curr_batch_size = real_images.size(0)\n",
    "\n",
    "        # Possibly create BCE labels if using BCE\n",
    "        if loss_option == \"bce\":\n",
    "            \n",
    "            labels_real = torch.full((curr_batch_size, 1), 0.9, device=device)\n",
    "            labels_fake = torch.full((curr_batch_size, 1), 0.1, device=device)\n",
    "\n",
    "        # Zero grad for D\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # Optionally add small noise to real images\n",
    "        # real_noisy = real_images + torch.randn_like(real_images) * 0.01\n",
    "\n",
    "        # ----- 1(a): Discriminator loss on real\n",
    "        if loss_option == \"w\":\n",
    "            output_real = D_attention(real_images).view(-1)\n",
    "            loss_d_real = -output_real.mean()\n",
    "        else:  # BCE\n",
    "            output_real = D_attention(real_images).view(-1, 1)\n",
    "            loss_d_real = criterion(output_real, labels_real)\n",
    "\n",
    "        # ----- 1(b): Discriminator loss on fake\n",
    "        noise = torch.randn(curr_batch_size, latent_dim, image_size, image_size, device=device)\n",
    "        fake_images = G_attention(noise).detach()  # detach so G is not updated here\n",
    "\n",
    "        # fake_noisy = fake_images + torch.randn_like(fake_images) * 0.01\n",
    "\n",
    "        if loss_option == \"w\":\n",
    "            output_fake = D_attention(fake_images).view(-1)\n",
    "            loss_d_fake = output_fake.mean()\n",
    "        else:  # BCE\n",
    "            output_fake = D_attention(fake_images).view(-1, 1)\n",
    "            loss_d_fake = criterion(output_fake, labels_fake)\n",
    "\n",
    "        # ----- 1(c): Gradient Penalty\n",
    "        gp = gradient_penalty(D_attention, real_images, fake_images, device=device) \n",
    "        # gradient_penalty(...) is assumed to return scalar\n",
    "\n",
    "        # Combine D losses\n",
    "        loss_d = loss_d_real + loss_d_fake + lambda_gp * gp\n",
    "        \n",
    "        # Backprop and step\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        # Track critic loss\n",
    "        critic_loss += loss_d.item()\n",
    "        critic_count += 1\n",
    "\n",
    "\n",
    "        ################################################################\n",
    "        # 2) GENERATOR UPDATE (every k steps of critic)\n",
    "        ################################################################\n",
    "        \n",
    "        if critic_count % k == 0: # Only update G every k steps\n",
    "            # Freeze D's parameters\n",
    "            for p in D_attention.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # We'll generate a new batch of noise/images\n",
    "            noise = torch.randn(batch_size, latent_dim, image_size, image_size, device=device)\n",
    "            fake_images = G_attention(noise)\n",
    "            \n",
    "            # Zero grad for G\n",
    "            optimizer_g.zero_grad()\n",
    "\n",
    "            if loss_option == \"w\":\n",
    "                output_fake_for_gen = D_attention(fake_images).view(-1)\n",
    "                loss_g = -output_fake_for_gen.mean()\n",
    "            else:  # BCE\n",
    "                labels_real = torch.full((batch_size, 1), 0.9, device=device)\n",
    "                output_fake_for_gen = D_attention(fake_images).view(-1, 1)\n",
    "                loss_g = criterion(output_fake_for_gen, labels_real)\n",
    "\n",
    "            # Diversity penalty (optional)\n",
    "            diversity_pen = diversity_loss(fake_images, feature_extractor)\n",
    "            loss_g += lambda_div * diversity_pen\n",
    "\n",
    "            # Backprop and step\n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            # Re-enable D grads for next iteration\n",
    "            for p in D_attention.parameters():\n",
    "                p.requires_grad = True\n",
    "                \n",
    "            # Track generator loss\n",
    "            generator_loss += loss_g.item()\n",
    "            generator_count += 1\n",
    "\n",
    "\n",
    "    # Evaluate FID every `epoch_eval` epochs\n",
    "    if epoch % epoch_fid == 0:\n",
    "        G_attention.eval()  # Set generator to eval mode for FID computation\n",
    "        with torch.no_grad():\n",
    "        # Generate evaluation images\n",
    "            noise_eval = torch.randn(n_samples, latent_dim, image_size, image_size, device=device)\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                fake_images_chunk = G_attention(noise_eval[i:i+batch_size]).to('cpu')\n",
    "                fid.update(fake_images_chunk, real=False)\n",
    "\n",
    "            fid_value = fid.compute().item()\n",
    "            FID_values.append(fid_value)\n",
    "\n",
    "            print(f'FID after epoch {epoch}: {fid_value}')\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        fid.reset()\n",
    "        G_attention.train()\n",
    "        \n",
    "    if epoch % epoch_sampling == 0:\n",
    "        G_attention.eval()  # Set generator to eval mode for FID computation\n",
    "        \n",
    "        # Generate evaluation images\n",
    "        with torch.no_grad():\n",
    "            noise_eval = torch.randn(64, latent_dim, image_size, image_size, device=device)\n",
    "            fake_images_eval = G_attention(noise_eval).to(torch.device('cpu'))\n",
    "        \n",
    "        # Save a grid of generated samples\n",
    "        grid = vutils.make_grid(fake_images_eval, normalize=True, scale_each=True)\n",
    "        sample_path = os.path.join(output_dir, f\"epoch_{epoch:03d}.png\")\n",
    "        vutils.save_image(grid, sample_path)\n",
    "        print(f\"Sample images saved to {sample_path}\")\n",
    "        \n",
    "        del fake_images_eval\n",
    "        torch.cuda.empty_cache()\n",
    "        G_attention.train()\n",
    "        \n",
    "    # Track losses\n",
    "    \n",
    "    generator_loss = generator_loss/generator_count\n",
    "    critic_loss = critic_loss/critic_count\n",
    "    \n",
    "    D_loss.append(critic_loss)\n",
    "    G_loss.append(generator_loss)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f'Epoch [{epoch }/{epochs}] | Loss D: {critic_loss} | Loss G: {generator_loss} | Wasserstein Distance: { - critic_loss - generator_loss}')\n",
    "    print(f'generator_count: {generator_count}, critic_count: {critic_count}')\n",
    "    print(f'Epoch duration: {end_time - start_time:.2f}s')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f\"weights/wgan/experiment_{experiment}\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "torch.save(D_attention.state_dict(), f\"{model_dir}/d_att.pth\")\n",
    "torch.save(G_attention.state_dict(), f\"{model_dir}/g_att.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 3\n",
    "latent_dim = 128\n",
    "channels_out = 3\n",
    "G_unet_test = AttentionUNetGenerator(latent_dim, channels_out)\n",
    "G_unet_test.load_state_dict(torch.load(f\"{model_dir}/g_att.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Generating some examples using the trained generator </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "noise_eval = torch.randn(64, latent_dim, image_size, image_size, device=torch.device('cpu'))\n",
    "G_unet_test.eval()  # Set generator to eval mode for sampling\n",
    "fake_images_eval = G_unet_test(noise_eval).to(torch.device('cpu'))\n",
    "\n",
    "# Save a grid of generated samples\n",
    "grid = vutils.make_grid(fake_images_eval, normalize=True, scale_each=True)\n",
    "np_grid = grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Display the images\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(np_grid)\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Plotting Losses <strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming G_loss and D_loss are defined\n",
    "ng = len(G_loss)\n",
    "nd = len(D_loss)\n",
    "time_steps_g = [i for i in range(ng)]\n",
    "time_steps_d = [i for i in range(nd)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Generator Loss\n",
    "plt.plot(time_steps_g, G_loss, label='Generator Loss', color='darkorange', linestyle='-', linewidth=2)\n",
    "\n",
    "# Plot Discriminator Loss\n",
    "plt.plot(time_steps_d, D_loss, label='Discriminator Loss', color='royalblue', linestyle='--', linewidth=2)\n",
    "\n",
    "# Add grid, labels, and title\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xlabel('Time Steps', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Loss Evolution of Generator and Discriminator', fontsize=14)\n",
    "\n",
    "# Adding legend\n",
    "plt.legend(loc='upper right', fontsize=11)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Plotting FID values <strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(FID_values)\n",
    "time_steps = [i for i in range(n)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot FID values\n",
    "plt.plot(time_steps, FID_values, label='Generator FID', color='darkorange', linestyle='-', linewidth=2)\n",
    "\n",
    "\n",
    "# Add grid, labels, and title\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xlabel('Time Steps', fontsize=12)\n",
    "plt.ylabel('FID', fontsize=12)\n",
    "plt.title('FID Evolution through training', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save G_loss and D_loss to text files\n",
    "plots_dir = f'plots/wgan/experiment_{experiment}'\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "np.savetxt(f'{plots_dir}/g_loss.txt', np.array(G_loss))\n",
    "np.savetxt(f'{plots_dir}/d_loss.txt', np.array(D_loss))\n",
    "\n",
    "# Save FID values to text file\n",
    "np.savetxt(f'{plots_dir}/fid_values.txt', np.array(FID_values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
